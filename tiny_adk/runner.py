"""Runner - Agent çš„æ‰§è¡Œå¼•æ“"""

from __future__ import annotations

import json
from typing import Any, Iterator

from .agents import Agent
from .config import Config, get_config
from .events import Event, EventType
from .session import Session
from .tools import Tool


class Runner:
  """
  Runner - æ— çŠ¶æ€çš„æ‰§è¡Œå¼•æ“
  
  æ ¸å¿ƒè®¾è®¡ç†å¿µ:
  - Runner è´Ÿè´£ç¼–æ’æ‰§è¡Œæµç¨‹
  - Runner ä¸ä¿å­˜çŠ¶æ€ï¼Œæ‰€æœ‰çŠ¶æ€åœ¨ Session ä¸­
  - Runner ç®¡ç† "Reason-Act" å¾ªç¯ï¼š
    1. ä» Session åŠ è½½å†å²
    2. è°ƒç”¨ LLM æ¨ç†
    3. æ‰§è¡Œå·¥å…·è°ƒç”¨
    4. ä¿å­˜äº‹ä»¶åˆ° Session
    5. é‡å¤ç›´åˆ°å®Œæˆ
  
  è¿™ç§è®¾è®¡ä½¿å¾—:
  - Runner å¯ä»¥æ˜¯å•ä¾‹æˆ–çŸ­ç”Ÿå‘½å‘¨æœŸå¯¹è±¡
  - Session å¯ä»¥è·¨ Runner å®ä¾‹æŒä¹…åŒ–
  - å®¹æ˜“å®ç°åˆ†å¸ƒå¼æ‰§è¡Œ
  """
  
  def __init__(
      self,
      llm_client: Any | None = None,
      api_base: str | None = None,
      api_key: str | None = None,
      default_model: str | None = None,
      show_thinking: bool | None = None,
      show_request: bool | None = None,
      config: Config | None = None,
  ):
    """
    åˆå§‹åŒ– Runner
    
    é…ç½®ä¼˜å…ˆçº§: ç›´æ¥ä¼ å…¥çš„å‚æ•° > ä¼ å…¥çš„ config > å…¨å±€é…ç½® (ç¯å¢ƒå˜é‡/é…ç½®æ–‡ä»¶)
    
    Args:
      llm_client: LLM å®¢æˆ·ç«¯ï¼ˆOpenAI å…¼å®¹çš„å®¢æˆ·ç«¯ï¼‰
      api_base: vLLM server çš„ API åœ°å€ï¼ˆå¦‚ "http://localhost:8000/v1"ï¼‰
      api_key: API å¯†é’¥ï¼ˆvLLM é€šå¸¸ç”¨ "EMPTY"ï¼‰
      default_model: é»˜è®¤æ¨¡å‹åç§°
      show_thinking: æ˜¯å¦æ‰“å°æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹ï¼ˆé»˜è®¤ Falseï¼‰
      show_request: æ˜¯å¦æ‰“å°è¯·æ±‚å‚æ•°ï¼ˆé»˜è®¤ Falseï¼‰
      config: å¯é€‰çš„é…ç½®å¯¹è±¡ï¼Œä¸ä¼ åˆ™ä½¿ç”¨å…¨å±€é…ç½®
    """
    # è·å–é…ç½®ï¼ˆä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ï¼Œå¦åˆ™ä½¿ç”¨å…¨å±€é…ç½®ï¼‰
    self._config = config or get_config()
    
    self.llm_client = llm_client
    # ä½¿ç”¨é…ç½®æ–‡ä»¶çš„å€¼ä½œä¸ºé»˜è®¤ï¼Œå‚æ•°å¯ä»¥è¦†ç›–
    self.api_base = api_base if api_base is not None else self._config.llm.api_base
    self.api_key = api_key if api_key is not None else self._config.llm.api_key
    self.default_model = default_model if default_model is not None else self._config.llm.model
    self.show_thinking = show_thinking if show_thinking is not None else self._config.runner.show_thinking
    self.show_request = show_request if show_request is not None else self._config.runner.show_request
    
    # å¦‚æœæ²¡æœ‰æä¾› clientï¼Œè‡ªåŠ¨åˆ›å»º
    if not llm_client and self.api_base:
      self._init_openai_client()
  
  def run(
      self,
      agent: Agent,
      session: Session,
      user_message: str,
  ) -> str:
    """
    åŒæ­¥æ‰§è¡Œä¸€è½®å¯¹è¯
    
    Args:
      agent: è¦æ‰§è¡Œçš„ Agent
      session: ä¼šè¯å¯¹è±¡
      user_message: ç”¨æˆ·æ¶ˆæ¯
    
    Returns:
      Agent çš„æœ€ç»ˆå“åº”
    """
    # 1. è®°å½•ç”¨æˆ·æ¶ˆæ¯äº‹ä»¶
    session.add_event(Event(
        event_type=EventType.USER_MESSAGE,
        content=user_message,
    ))
    
    # 2. æ‰§è¡Œ Reason-Act å¾ªç¯
    response = self._reason_act_loop(agent, session)
    
    return response
  
  def run_stream(
      self,
      agent: Agent,
      session: Session,
      user_message: str,
  ) -> Iterator[Event]:
    """
    æµå¼æ‰§è¡Œ - å®æ—¶è¿”å›äº‹ä»¶
    
    åªè¿”å› Agent çš„å“åº”å’Œå·¥å…·è°ƒç”¨äº‹ä»¶ï¼Œä¸è¿”å›ç”¨æˆ·æ¶ˆæ¯
    ç”¨æˆ·æ¶ˆæ¯ä¼šè¢«è®°å½•åˆ° session ä¸­ï¼Œä½†ä¸ä¼š yield
    """
    # 1. è®°å½•ç”¨æˆ·æ¶ˆæ¯åˆ° sessionï¼ˆä¸ yieldï¼‰
    session.add_event(Event(
        event_type=EventType.USER_MESSAGE,
        content=user_message,
    ))
    
    # 2. æ‰§è¡Œå¹¶æµå¼è¿”å› Agent çš„äº‹ä»¶
    yield from self._reason_act_loop_stream(agent, session)
  
  def _reason_act_loop(
      self,
      agent: Agent,
      session: Session,
  ) -> str:
    """
    Reason-Act å¾ªç¯çš„ç®€åŒ–å®ç°
    
    çœŸå®çš„ ADK å®ç°ä¼šï¼š
    - è°ƒç”¨å®é™…çš„ LLM API
    - å¤„ç†å‡½æ•°è°ƒç”¨
    - å¤„ç†é”™è¯¯å’Œé‡è¯•
    - æ”¯æŒå¤šè½®å·¥å…·è°ƒç”¨
    
    è¿™é‡Œç”¨ç®€åŒ–çš„é€»è¾‘å±•ç¤ºæ ¸å¿ƒæµç¨‹
    """
    # æ„å»ºè¯·æ±‚
    messages = self._build_messages(agent, session)
    
    # æ¨¡æ‹Ÿ LLM è°ƒç”¨ï¼ˆå®é™…åº”è¯¥è°ƒç”¨ self.llm_clientï¼‰
    if self.llm_client is None:
      response = self._mock_llm_response(agent, messages)
    else:
      response = self._call_llm(agent, messages)
    
    # è®°å½•æ¨¡å‹å“åº”
    session.add_event(Event(
        event_type=EventType.MODEL_RESPONSE,
        content=response['content'],
        metadata=response.get('metadata', {}),
    ))
    
    # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œæ‰§è¡Œå·¥å…·
    if 'tool_calls' in response:
      for tool_call in response['tool_calls']:
        self._execute_tool(agent, session, tool_call)
      
      # é€’å½’ç»§ç»­å¾ªç¯ï¼ˆå·¥å…·æ‰§è¡Œåè®© LLM ç»§ç»­ï¼‰
      return self._reason_act_loop(agent, session)
    
    return response['content']
  
  def _reason_act_loop_stream(
      self,
      agent: Agent,
      session: Session,
  ) -> Iterator[Event]:
    """æµå¼ç‰ˆæœ¬çš„ Reason-Act å¾ªç¯ - çœŸæ­£çš„æµå¼è¾“å‡º"""
    messages = self._build_messages(agent, session)
    
    # ä½¿ç”¨æµå¼è°ƒç”¨æˆ–æ¨¡æ‹Ÿå“åº”
    if self.llm_client is None:
      # æ¨¡æ‹Ÿæµå¼å“åº”
      response = self._mock_llm_response(agent, messages)
      event = Event(
          event_type=EventType.MODEL_RESPONSE,
          content=response['content'],
          metadata=response.get('metadata', {}),
      )
      session.add_event(event)
      yield event
    else:
      # çœŸæ­£çš„æµå¼ LLM è°ƒç”¨
      full_content = ''
      full_response = None
      
      # é€æ­¥æ¥æ”¶æµå¼å“åº”
      for chunk in self._call_llm_stream(agent, messages):
        if chunk.get('type') == 'content':
          # æµå¼å†…å®¹ç‰‡æ®µ
          full_content += chunk['delta']
          # å®æ—¶ yield å†…å®¹äº‹ä»¶ï¼ˆå¯é€‰ï¼šç”¨äºå®æ—¶æ˜¾ç¤ºï¼‰
          yield Event(
              event_type=EventType.MODEL_RESPONSE_DELTA,
              content=chunk['delta'],
              metadata={'type': 'delta'},
          )
        elif chunk.get('done'):
          # å®Œæ•´å“åº”
          full_response = chunk
      
      # ä¿å­˜å®Œæ•´å“åº”åˆ° session
      if full_response:
        response = full_response
        event = Event(
            event_type=EventType.MODEL_RESPONSE,
            content=response['content'],
            metadata=response.get('metadata', {}),
        )
        session.add_event(event)
        yield event
      else:
        # å¦‚æœæ²¡æœ‰æ”¶åˆ°å®Œæ•´å“åº”ï¼Œä½¿ç”¨ç´¯ç§¯çš„å†…å®¹
        response = {'content': full_content}
        event = Event(
            event_type=EventType.MODEL_RESPONSE,
            content=full_content,
            metadata={},
        )
        session.add_event(event)
        yield event
    
    # å¤„ç†å·¥å…·è°ƒç”¨
    if 'tool_calls' in response:
      for tool_call in response['tool_calls']:
        yield from self._execute_tool_stream(agent, session, tool_call)
      
      # ç»§ç»­å¾ªç¯
      yield from self._reason_act_loop_stream(agent, session)
  
  def _build_messages(
      self,
      agent: Agent,
      session: Session,
  ) -> list[dict[str, Any]]:
    """
    æ„å»ºå‘é€ç»™ LLM çš„æ¶ˆæ¯åˆ—è¡¨
    
    æ ¸å¿ƒè½¬æ¢ï¼šSession Events -> LLM Messages
    """
    messages = [
        {'role': 'system', 'content': agent.get_system_prompt()}
    ]
    
    # æ·»åŠ å†å²å¯¹è¯
    messages.extend(session.get_conversation_history())
    
    return messages
  
  def _init_openai_client(self):
    """åˆå§‹åŒ– OpenAI å…¼å®¹çš„å®¢æˆ·ç«¯"""
    try:
      from openai import OpenAI
      self.llm_client = OpenAI(
          base_url=self.api_base,
          api_key=self.api_key,
      )
      print(f"âœ… å·²è¿æ¥åˆ° LLM: {self.api_base}")
    except ImportError:
      raise ImportError(
          "éœ€è¦å®‰è£… openai åŒ…: pip install openai"
      )
  
  def _extract_thinking_content(self, raw_content: str) -> tuple[str, str]:
    """
    æå–å¹¶åˆ†ç¦»æ€è€ƒå†…å®¹
    
    å°† <think>...</think> æ ‡ç­¾å†…çš„å†…å®¹æå–å‡ºæ¥ï¼Œä¸æ”¾å…¥å¯¹è¯å†å²
    è¿™ç¬¦åˆä¸»æµè®¾è®¡ï¼ˆADK/Anthropic/OpenAIï¼‰çš„åšæ³•
    
    Args:
      raw_content: åŸå§‹æ¨¡å‹è¾“å‡º
    
    Returns:
      (clean_content, thinking_content) - æ¸…æ´—åçš„å†…å®¹å’Œæ€è€ƒè¿‡ç¨‹
    """
    import re
    
    if not raw_content:
      return '', ''
    
    # æå– <think> æ ‡ç­¾å†…å®¹
    think_pattern = r'<think>(.*?)</think>'
    thinking_parts = re.findall(think_pattern, raw_content, re.DOTALL)
    
    # ç§»é™¤ <think> æ ‡ç­¾ï¼Œåªä¿ç•™å®é™…è¾“å‡º
    clean_content = re.sub(think_pattern, '', raw_content, flags=re.DOTALL).strip()
    
    # åˆå¹¶æ‰€æœ‰æ€è€ƒå†…å®¹
    thinking_content = '\n'.join(part.strip() for part in thinking_parts) if thinking_parts else ''
    
    return clean_content, thinking_content
  
  class _StreamThinkingFilter:
    """æµå¼æ€è€ƒå†…å®¹è¿‡æ»¤å™¨ - å®æ—¶è¿‡æ»¤ <think> æ ‡ç­¾"""
    
    def __init__(self):
      self.buffer = ''  # ç¼“å†²åŒº
      self.in_thinking = False  # æ˜¯å¦åœ¨æ€è€ƒæ¨¡å¼
      self.thinking_content = ''  # ç´¯ç§¯çš„æ€è€ƒå†…å®¹
      self.clean_content = ''  # æ¸…æ´—åçš„å†…å®¹
    
    def process_delta(self, delta: str) -> str:
      """
      å¤„ç†æµå¼å†…å®¹ç‰‡æ®µï¼Œè¿‡æ»¤ thinking å†…å®¹
      
      Returns:
        åº”è¯¥è¾“å‡ºçš„å†…å®¹ï¼ˆå¯èƒ½ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰
      """
      self.buffer += delta
      output = ''
      
      while self.buffer:
        if not self.in_thinking:
          # ä¸åœ¨æ€è€ƒæ¨¡å¼ï¼Œæ£€æŸ¥æ˜¯å¦é‡åˆ° <think> æ ‡ç­¾
          think_start_idx = self.buffer.find('<think>')
          
          if think_start_idx == -1:
            # æ²¡æœ‰ <think>ï¼Œä½†å¯èƒ½åœ¨æœ«å°¾æœ‰éƒ¨åˆ†æ ‡ç­¾ï¼Œä¿ç•™å°‘é‡ç¼“å†²
            if len(self.buffer) > 10:
              # è¾“å‡ºé™¤äº†æœ€åå‡ ä¸ªå­—ç¬¦å¤–çš„æ‰€æœ‰å†…å®¹
              output_part = self.buffer[:-7]  # ä¿ç•™ 7 ä¸ªå­—ç¬¦ï¼ˆ'<think>' çš„é•¿åº¦ï¼‰
              output += output_part
              self.clean_content += output_part
              self.buffer = self.buffer[-7:]
            break
          else:
            # æ‰¾åˆ° <think>ï¼Œè¾“å‡ºä¹‹å‰çš„å†…å®¹
            if think_start_idx > 0:
              output_part = self.buffer[:think_start_idx]
              output += output_part
              self.clean_content += output_part
            # è¿›å…¥æ€è€ƒæ¨¡å¼ï¼Œè·³è¿‡ <think> æ ‡ç­¾
            self.buffer = self.buffer[think_start_idx + 7:]
            self.in_thinking = True
        else:
          # åœ¨æ€è€ƒæ¨¡å¼ï¼ŒæŸ¥æ‰¾ </think> æ ‡ç­¾
          think_end_idx = self.buffer.find('</think>')
          
          if think_end_idx == -1:
            # æ²¡æœ‰ </think>ï¼Œä¿ç•™å°‘é‡ç¼“å†²
            if len(self.buffer) > 10:
              thinking_part = self.buffer[:-8]  # ä¿ç•™ 8 ä¸ªå­—ç¬¦ï¼ˆ'</think>' çš„é•¿åº¦ï¼‰
              self.thinking_content += thinking_part
              self.buffer = self.buffer[-8:]
            break
          else:
            # æ‰¾åˆ° </think>ï¼Œä¿å­˜æ€è€ƒå†…å®¹
            self.thinking_content += self.buffer[:think_end_idx]
            # é€€å‡ºæ€è€ƒæ¨¡å¼ï¼Œè·³è¿‡ </think> æ ‡ç­¾
            self.buffer = self.buffer[think_end_idx + 8:]
            self.in_thinking = False
      
      return output
    
    def finalize(self) -> tuple[str, str, str]:
      """
      å®Œæˆå¤„ç†ï¼Œè¿”å›æ¸…æ´—åçš„å†…å®¹ã€æ€è€ƒå†…å®¹å’Œå‰©ä½™ç¼“å†²
      
      Returns:
        (clean_content, thinking_content, remaining_buffer)
      """
      remaining = ''
      
      # å¤„ç†å‰©ä½™ç¼“å†²åŒº
      if self.buffer:
        if self.in_thinking:
          self.thinking_content += self.buffer
        else:
          # éæ€è€ƒæ¨¡å¼çš„ç¼“å†²åŒºå†…å®¹åº”è¯¥è¾“å‡º
          remaining = self.buffer
          self.clean_content += self.buffer
        self.buffer = ''
      
      return self.clean_content.strip(), self.thinking_content.strip(), remaining
  
  def _call_llm_stream(
      self,
      agent: Agent,
      messages: list[dict[str, Any]],
  ) -> Iterator[dict[str, Any]]:
    """
    æµå¼è°ƒç”¨ LLM - å®æ—¶è¿”å›ç”Ÿæˆçš„å†…å®¹ç‰‡æ®µ
    
    Yields:
      åŒ…å« 'delta' (å†…å®¹ç‰‡æ®µ) æˆ– 'done' (å®Œæ•´å“åº”) çš„å­—å…¸
    """
    if not self.llm_client:
      raise ValueError(
          "æœªé…ç½® LLM å®¢æˆ·ç«¯ã€‚è¯·åœ¨åˆå§‹åŒ– Runner æ—¶æä¾› llm_client æˆ– api_base"
      )
    
    # å‡†å¤‡å·¥å…·å®šä¹‰ï¼ˆå¦‚æœæœ‰ï¼‰
    tools = None
    if agent.tools:
      tools = [self._tool_to_openai_format(tool) for tool in agent.tools]
    
    try:
      # ä½¿ç”¨ agent.modelï¼Œå¦‚æœä¸ºç©ºæˆ–æ˜¯é»˜è®¤å€¼ 'gpt-4'ï¼Œåˆ™ä½¿ç”¨ runner çš„é»˜è®¤æ¨¡å‹
      model_to_use = agent.model
      if not model_to_use or model_to_use == 'gpt-4':
        model_to_use = self.default_model
      
      # æ„å»ºè¯·æ±‚å‚æ•°
      request_params = {
          'model': model_to_use,
          'messages': messages,
          'temperature': agent.temperature,
          'max_tokens': agent.max_tokens,
          'stream': True,  # å¯ç”¨æµå¼æ¨¡å¼
      }
      
      # å¯é€‰ï¼šæ‰“å°è¯·æ±‚å‚æ•°
      if self.show_request:
        print('--------------------------------')
        print('LLM æµå¼è¯·æ±‚å‚æ•°:')
        print({**request_params, 'stream': True})
        print('--------------------------------')
      
      # å¦‚æœæœ‰å·¥å…·ï¼Œæ·»åŠ å·¥å…·å®šä¹‰
      if tools:
        request_params['tools'] = tools
        request_params['tool_choice'] = 'auto'
      
      # æµå¼è°ƒç”¨ API
      stream = self.llm_client.chat.completions.create(**request_params)
      
      # æ”¶é›†å®Œæ•´å“åº”
      full_content = ''
      tool_calls_data = []
      finish_reason = None
      model_name = None
      
      # åˆ›å»ºæ€è€ƒå†…å®¹è¿‡æ»¤å™¨ï¼ˆç”¨äºæ¸…æ´—å¯¹è¯å†å²ï¼‰
      thinking_filter = self._StreamThinkingFilter()
      
      # é€æ­¥å¤„ç†æµå¼å“åº”
      for chunk in stream:
        if not chunk.choices:
          continue
        
        choice = chunk.choices[0]
        delta = choice.delta
        
        # ä¿å­˜æ¨¡å‹åç§°
        if chunk.model:
          model_name = chunk.model
        
        # å¤„ç†å†…å®¹ç‰‡æ®µ
        if delta.content:
          full_content += delta.content
          
          # å§‹ç»ˆé€šè¿‡è¿‡æ»¤å™¨å¤„ç†ï¼ˆç”¨äºç”Ÿæˆæ¸…æ´—åçš„å†…å®¹ä¿å­˜åˆ° sessionï¼‰
          filtered_delta = thinking_filter.process_delta(delta.content)
          
          # æ ¹æ® show_thinking å†³å®šè¾“å‡ºå†…å®¹
          if self.show_thinking:
            # æ˜¾ç¤º thinkingï¼šåŸæ ·è¾“å‡ºæ‰€æœ‰å†…å®¹
            yield {
                'delta': delta.content,  # åŸå§‹å†…å®¹ï¼ŒåŒ…å« <think> æ ‡ç­¾
                'type': 'content',
            }
          else:
            # ä¸æ˜¾ç¤º thinkingï¼šåªè¾“å‡ºè¿‡æ»¤åçš„å†…å®¹
            if filtered_delta:
              yield {
                  'delta': filtered_delta,
                  'type': 'content',
              }
        
        # å¤„ç†å·¥å…·è°ƒç”¨
        if delta.tool_calls:
          for tc in delta.tool_calls:
            tool_calls_data.append({
                'id': tc.id,
                'name': tc.function.name if tc.function else None,
                'arguments': tc.function.arguments if tc.function else None,
            })
        
        # å¤„ç†å®ŒæˆåŸå› 
        if choice.finish_reason:
          finish_reason = choice.finish_reason
      
      # å®Œæˆè¿‡æ»¤ï¼Œè·å–æ¸…æ´—åçš„å†…å®¹ã€æ€è€ƒå†…å®¹å’Œå‰©ä½™ç¼“å†²
      clean_content, thinking, remaining = thinking_filter.finalize()
      
      # å¦‚æœä¸æ˜¾ç¤º thinkingï¼Œéœ€è¦è¾“å‡ºå‰©ä½™ç¼“å†²åŒºå†…å®¹ï¼ˆæœ€åå‡ ä¸ªå­—ç¬¦ï¼‰
      # å¦‚æœæ˜¾ç¤º thinkingï¼Œå‰©ä½™å†…å®¹å·²ç»åœ¨ä¸Šé¢åŸæ ·è¾“å‡ºäº†
      if not self.show_thinking and remaining:
        yield {
            'delta': remaining,
            'type': 'content',
        }
      
      # è¿”å›å®Œæ•´å“åº”
      result = {
          'done': True,
          'content': clean_content,
          'raw_content': full_content,
          'metadata': {
              'model': model_name or model_to_use,
              'finish_reason': finish_reason,
              'thinking': thinking,
          },
      }
      
      # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
      if tool_calls_data:
        # åˆå¹¶å·¥å…·è°ƒç”¨æ•°æ®
        merged_tool_calls = []
        for tc in tool_calls_data:
          if tc.get('name'):
            merged_tool_calls.append({
                'id': tc.get('id', 'call_unknown'),
                'name': tc['name'],
                'arguments': json.loads(tc.get('arguments', '{}')),
            })
        
        if merged_tool_calls:
          result['tool_calls'] = merged_tool_calls
      
      yield result
    
    except Exception as e:
      # é”™è¯¯å¤„ç†
      yield {
          'done': True,
          'content': f"LLM æµå¼è°ƒç”¨å¤±è´¥: {str(e)}",
          'metadata': {'error': str(e)},
      }
  
  def _call_llm(
      self,
      agent: Agent,
      messages: list[dict[str, Any]],
  ) -> dict[str, Any]:
    """
    è°ƒç”¨ OpenAI å…¼å®¹çš„ LLMï¼ˆvLLM serverï¼‰- éæµå¼ç‰ˆæœ¬
    
    æ”¯æŒ:
    - æ–‡æœ¬ç”Ÿæˆ
    - å‡½æ•°è°ƒç”¨ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
    - æ€è€ƒå†…å®¹åˆ†ç¦»ï¼ˆä¸æ±¡æŸ“å¯¹è¯å†å²ï¼‰
    """
    if not self.llm_client:
      raise ValueError(
          "æœªé…ç½® LLM å®¢æˆ·ç«¯ã€‚è¯·åœ¨åˆå§‹åŒ– Runner æ—¶æä¾› llm_client æˆ– api_base"
      )
    
    # å‡†å¤‡å·¥å…·å®šä¹‰ï¼ˆå¦‚æœæœ‰ï¼‰
    tools = None
    if agent.tools:
      tools = [self._tool_to_openai_format(tool) for tool in agent.tools]
    
    # è°ƒç”¨ LLM
    try:
      # ä½¿ç”¨ agent.modelï¼Œå¦‚æœä¸ºç©ºæˆ–æ˜¯é»˜è®¤å€¼ 'gpt-4'ï¼Œåˆ™ä½¿ç”¨ runner çš„é»˜è®¤æ¨¡å‹
      model_to_use = agent.model
      if not model_to_use or model_to_use == 'gpt-4':
        model_to_use = self.default_model
      
      # æ„å»ºè¯·æ±‚å‚æ•°
      request_params = {
          'model': model_to_use,
          'messages': messages,
          'temperature': agent.temperature,
          'max_tokens': agent.max_tokens,
      }

      # å¯é€‰ï¼šæ‰“å°è¯·æ±‚å‚æ•°
      if self.show_request:
        print('--------------------------------')
        print('LLM è¯·æ±‚å‚æ•°:')
        print(request_params)
        print('--------------------------------')
      
      # å¦‚æœæœ‰å·¥å…·ï¼Œæ·»åŠ å·¥å…·å®šä¹‰
      if tools:
        request_params['tools'] = tools
        request_params['tool_choice'] = 'auto'
      
      # è°ƒç”¨ API
      response = self.llm_client.chat.completions.create(**request_params)
      
      # è§£æå“åº”
      choice = response.choices[0]
      message = choice.message
      
      # æå–åŸå§‹å†…å®¹
      raw_content = message.content or ''
      
      # åˆ†ç¦»æ€è€ƒå†…å®¹å’Œå®é™…è¾“å‡º
      clean_content, thinking = self._extract_thinking_content(raw_content)
      
      # å¯é€‰ï¼šæ‰“å°æ€è€ƒè¿‡ç¨‹ï¼ˆç”¨äºè°ƒè¯•ï¼‰
      if thinking and self.show_thinking:
        print('--------------------------------')
        print('ğŸ’­ Agent æ€è€ƒè¿‡ç¨‹:')
        print(thinking)
        print('--------------------------------')
      
      # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
      if message.tool_calls:
        return {
            'content': clean_content,  # åªä¿å­˜æ¸…æ´—åçš„å†…å®¹
            'tool_calls': [
                {
                    'id': tc.id,
                    'name': tc.function.name,
                    'arguments': json.loads(tc.function.arguments),
                }
                for tc in message.tool_calls
            ],
            'metadata': {
                'model': response.model,
                'finish_reason': choice.finish_reason,
                'thinking': thinking,  # æ€è€ƒè¿‡ç¨‹æ”¾åœ¨ metadata ä¸­
                'raw_content': raw_content,  # ä¿ç•™åŸå§‹å†…å®¹ç”¨äºè°ƒè¯•
            },
        }
      
      # æ™®é€šæ–‡æœ¬å“åº”
      return {
          'content': clean_content,  # åªä¿å­˜æ¸…æ´—åçš„å†…å®¹
          'metadata': {
              'model': response.model,
              'finish_reason': choice.finish_reason,
              'thinking': thinking,  # æ€è€ƒè¿‡ç¨‹æ”¾åœ¨ metadata ä¸­
              'raw_content': raw_content,  # ä¿ç•™åŸå§‹å†…å®¹ç”¨äºè°ƒè¯•
              'usage': {
                  'prompt_tokens': response.usage.prompt_tokens,
                  'completion_tokens': response.usage.completion_tokens,
                  'total_tokens': response.usage.total_tokens,
              } if response.usage else {},
          },
      }
    
    except Exception as e:
      # é”™è¯¯å¤„ç†
      return {
          'content': f"LLM è°ƒç”¨å¤±è´¥: {str(e)}",
          'metadata': {'error': str(e)},
      }
  
  def _tool_to_openai_format(self, tool: Tool) -> dict[str, Any]:
    """
    å°† Tool è½¬æ¢ä¸º OpenAI function calling æ ¼å¼
    
    OpenAI æ ¼å¼:
    {
      "type": "function",
      "function": {
        "name": "get_weather",
        "description": "è·å–å¤©æ°”",
        "parameters": {
          "type": "object",
          "properties": {
            "city": {"type": "string", "description": "åŸå¸‚å"}
          },
          "required": ["city"]
        }
      }
    }
    """
    # æ„å»ºå‚æ•°å®šä¹‰
    properties = {}
    required = []
    
    for param_name, param_info in tool.parameters.items():
      param_type = param_info.get('type', 'string')
      
      # è½¬æ¢ Python ç±»å‹åˆ° JSON Schema ç±»å‹
      type_mapping = {
          'str': 'string',
          'int': 'integer',
          'float': 'number',
          'bool': 'boolean',
          'list': 'array',
          'dict': 'object',
      }
      json_type = type_mapping.get(param_type, 'string')
      
      properties[param_name] = {
          'type': json_type,
          'description': param_info.get('description', f'å‚æ•° {param_name}'),
      }
      
      # å¦‚æœæ²¡æœ‰é»˜è®¤å€¼ï¼Œåˆ™ä¸ºå¿…éœ€å‚æ•°
      if 'default' not in param_info:
        required.append(param_name)
    
    return {
        'type': 'function',
        'function': {
            'name': tool.name,
            'description': tool.description,
            'parameters': {
                'type': 'object',
                'properties': properties,
                'required': required,
            },
        },
    }
  
  # Mock å“åº”æ ‡ç­¾
  MOCK_LABEL = "[MOCK]"
  
  def _mock_llm_response(
      self,
      agent: Agent,
      messages: list[dict[str, Any]],
  ) -> dict[str, Any]:
    """æ¨¡æ‹Ÿ LLM å“åº”ï¼ˆç”¨äºæ¼”ç¤ºï¼‰"""
    last_message = messages[-1] if messages else {}
    
    # æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯çš„è§’è‰²
    last_role = last_message.get('role', '')
    last_content = last_message.get('content', '')
    
    # Mock å…ƒæ•°æ®
    mock_metadata = {'model': agent.model, 'is_mock': True}
    
    # å¦‚æœä¸Šä¸€æ¡æ˜¯å·¥å…·å“åº”ï¼Œè¯´æ˜å·¥å…·å·²ç»æ‰§è¡Œå®Œæ¯•ï¼Œåº”è¯¥è¿”å›æœ€ç»ˆç­”æ¡ˆ
    if last_role == 'tool':
      tool_result = last_content
      return {
          'content': f"{self.MOCK_LABEL} æ ¹æ®æŸ¥è¯¢ç»“æœï¼š{tool_result}",
          'metadata': mock_metadata,
      }
    
    # å¦‚æœæ˜¯ç”¨æˆ·æ¶ˆæ¯ä¸”åŒ…å«ç‰¹å®šå…³é”®è¯ï¼Œæ¨¡æ‹Ÿå·¥å…·è°ƒç”¨
    if last_role == 'user' and agent.tools:
      content_str = str(last_content)
      
      # å¤©æ°”æŸ¥è¯¢
      if 'å¤©æ°”' in content_str:
        # æå–åŸå¸‚åï¼ˆç®€å•æ¨¡æ‹Ÿï¼‰
        city = 'åŒ—äº¬'  # é»˜è®¤
        for c in ['åŒ—äº¬', 'ä¸Šæµ·', 'æ·±åœ³', 'æˆéƒ½']:
          if c in content_str:
            city = c
            break
        
        return {
            'content': None,
            'tool_calls': [{
                'id': 'call_weather_123',
                'name': 'get_weather',
                'arguments': {'city': city},
            }],
            'metadata': mock_metadata,
        }
      
      # è®¡ç®—è¯·æ±‚
      if 'è®¡ç®—' in content_str or '=' in content_str:
        # ç®€å•æå–è¡¨è¾¾å¼
        import re
        expr_match = re.search(r'(\d+\s*[\+\-\*/]\s*\d+)', content_str)
        if expr_match:
          expression = expr_match.group(1).replace(' ', '')
          return {
              'content': None,
              'tool_calls': [{
                  'id': 'call_calc_123',
                  'name': 'calculate',
                  'arguments': {'expression': expression},
              }],
              'metadata': mock_metadata,
          }
      
      # æœç´¢è¯·æ±‚
      if 'æœç´¢' in content_str or 'æŸ¥æ‰¾' in content_str:
        return {
            'content': None,
            'tool_calls': [{
                'id': 'call_search_123',
                'name': 'web_search',
                'arguments': {'query': content_str},
            }],
            'metadata': mock_metadata,
        }
    
    # é»˜è®¤æ–‡æœ¬å“åº”
    return {
        'content': f"{self.MOCK_LABEL} æˆ‘æ˜¯ {agent.name}ã€‚æ”¶åˆ°æ¶ˆæ¯: {last_content}",
        'metadata': mock_metadata,
    }
  
  def _execute_tool(
      self,
      agent: Agent,
      session: Session,
      tool_call: dict[str, Any],
  ) -> None:
    """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
    # è®°å½•å·¥å…·è°ƒç”¨äº‹ä»¶
    session.add_event(Event(
        event_type=EventType.TOOL_CALL,
        content=tool_call,
    ))
    
    # æŸ¥æ‰¾å¹¶æ‰§è¡Œå·¥å…·
    tool = self._find_tool(agent, tool_call['name'])
    if tool:
      try:
        # è§£æå‚æ•°
        args = tool_call.get('arguments', {})
        if isinstance(args, str):
          args = json.loads(args)
        
        # æ‰§è¡Œå·¥å…·
        result = tool.execute(**args)
        
        # è®°å½•å·¥å…·å“åº”
        session.add_event(Event(
            event_type=EventType.TOOL_RESPONSE,
            content={
                'call_id': tool_call.get('id'),
                'name': tool_call['name'],
                'result': str(result),
            },
        ))
      except Exception as e:
        # è®°å½•é”™è¯¯
        session.add_event(Event(
            event_type=EventType.ERROR,
            content={
                'tool': tool_call['name'],
                'error': str(e),
            },
        ))
  
  def _execute_tool_stream(
      self,
      agent: Agent,
      session: Session,
      tool_call: dict[str, Any],
  ) -> Iterator[Event]:
    """æµå¼æ‰§è¡Œå·¥å…·"""
    # å·¥å…·è°ƒç”¨äº‹ä»¶
    event = Event(event_type=EventType.TOOL_CALL, content=tool_call)
    session.add_event(event)
    yield event
    
    # æ‰§è¡Œå·¥å…·
    tool = self._find_tool(agent, tool_call['name'])
    if tool:
      try:
        args = tool_call.get('arguments', {})
        if isinstance(args, str):
          args = json.loads(args)
        
        result = tool.execute(**args)
        
        event = Event(
            event_type=EventType.TOOL_RESPONSE,
            content={
                'call_id': tool_call.get('id'),
                'name': tool_call['name'],
                'result': str(result),
            },
        )
        session.add_event(event)
        yield event
      except Exception as e:
        event = Event(
            event_type=EventType.ERROR,
            content={'tool': tool_call['name'], 'error': str(e)},
        )
        session.add_event(event)
        yield event
  
  def _find_tool(self, agent: Agent, tool_name: str) -> Tool | None:
    """æŸ¥æ‰¾å·¥å…·"""
    for tool in agent.tools:
      if tool.name == tool_name:
        return tool
    return None

