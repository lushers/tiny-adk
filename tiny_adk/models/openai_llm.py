"""OpenAI å…¼å®¹çš„ LLM å®ç°"""

from __future__ import annotations

import asyncio
import json
import re
from typing import Any, AsyncIterator, Iterator

from .base_llm import BaseLlm
from .llm_request import LlmRequest
from .llm_response import LlmResponse, FunctionCall


class ThinkingFilter:
    """æµå¼æ€è€ƒå†…å®¹è¿‡æ»¤å™¨ - å®æ—¶è¿‡æ»¤ <think> æ ‡ç­¾"""
    
    def __init__(self):
        self.buffer = ''
        self.in_thinking = False
        self.thinking_content = ''
        self.clean_content = ''
    
    def process_delta(self, delta: str) -> str:
        """å¤„ç†æµå¼å†…å®¹ç‰‡æ®µï¼Œè¿‡æ»¤ thinking å†…å®¹"""
        self.buffer += delta
        output = ''
        
        while self.buffer:
            if not self.in_thinking:
                think_start_idx = self.buffer.find('<think>')
                
                if think_start_idx == -1:
                    if len(self.buffer) > 10:
                        output_part = self.buffer[:-7]
                        output += output_part
                        self.clean_content += output_part
                        self.buffer = self.buffer[-7:]
                    break
                else:
                    if think_start_idx > 0:
                        output_part = self.buffer[:think_start_idx]
                        output += output_part
                        self.clean_content += output_part
                    self.buffer = self.buffer[think_start_idx + 7:]
                    self.in_thinking = True
            else:
                think_end_idx = self.buffer.find('</think>')
                
                if think_end_idx == -1:
                    if len(self.buffer) > 10:
                        thinking_part = self.buffer[:-8]
                        self.thinking_content += thinking_part
                        self.buffer = self.buffer[-8:]
                    break
                else:
                    self.thinking_content += self.buffer[:think_end_idx]
                    self.buffer = self.buffer[think_end_idx + 8:]
                    self.in_thinking = False
        
        return output
    
    def finalize(self) -> tuple[str, str, str]:
        """è¿”å› (clean_content, thinking_content, remaining_buffer)"""
        remaining = ''
        
        if self.buffer:
            if self.in_thinking:
                self.thinking_content += self.buffer
            else:
                remaining = self.buffer
                self.clean_content += self.buffer
            self.buffer = ''
        
        return self.clean_content.strip(), self.thinking_content.strip(), remaining


class OpenAILlm(BaseLlm):
    """
    OpenAI å…¼å®¹çš„ LLM å®ç°
    
    ç»Ÿä¸€æ¥å£è®¾è®¡ï¼ˆå€Ÿé‰´ Google ADKï¼‰ï¼š
    - generate() å’Œ generate_async() éƒ½è¿”å›ç”Ÿæˆå™¨
    - é€šè¿‡ stream å‚æ•°åŒºåˆ†æµå¼/éæµå¼
    - éæµå¼åª yield ä¸€æ¬¡ï¼Œæµå¼ yield å¤šæ¬¡
    """
    
    api_base: str | None = None
    api_key: str | None = None
    show_thinking: bool = False
    show_request: bool = False
    log_level: str = "normal"  # minimal | normal | verbose
    
    _client: Any = None
    
    def model_post_init(self, __context: Any) -> None:
        """Pydantic åˆå§‹åŒ–å®Œæˆåçš„é’©å­"""
        super().model_post_init(__context)
        self._client = None
    
    @property
    def client(self) -> Any:
        """è·å– OpenAI å®¢æˆ·ç«¯ï¼ˆæ‡’åŠ è½½ï¼‰"""
        if self._client is None:
            from openai import OpenAI
            self._client = OpenAI(
                base_url=self.api_base,
                api_key=self.api_key,
            )
        return self._client
    
    @classmethod
    def supported_models(cls) -> list[str]:
        return [r"gpt-.*", r"o1-.*", r"chatgpt-.*"]
    
    # ==================== ç»Ÿä¸€ç”Ÿæˆæ¥å£ ====================
    
    def generate(
        self, 
        request: LlmRequest, 
        stream: bool = False,
    ) -> Iterator[LlmResponse]:
        """
        åŒæ­¥ç”Ÿæˆï¼ˆç»Ÿä¸€æ¥å£ï¼‰
        
        - stream=False: åª yield ä¸€æ¬¡å®Œæ•´å“åº”
        - stream=True: yield å¤šä¸ªå¢é‡ + æœ€åå®Œæ•´å“åº”
        """
        try:
            params = request.to_openai_format()
            params["model"] = self.get_model(request)
            params["stream"] = stream
            
            self._log_request(params)
            
            if stream:
                # æµå¼ç”Ÿæˆ
                stream_response = self.client.chat.completions.create(**params)
                yield from self._process_stream(stream_response)
            else:
                # éæµå¼ç”Ÿæˆ
                response = self.client.chat.completions.create(**params)
                result = self._parse_response(response)
                self._log_response(result)
                yield result
                
        except Exception as e:
            yield LlmResponse.from_error(str(e))
    
    async def generate_async(
        self, 
        request: LlmRequest, 
        stream: bool = False,
    ) -> AsyncIterator[LlmResponse]:
        """
        å¼‚æ­¥ç”Ÿæˆï¼ˆç»Ÿä¸€æ¥å£ï¼‰
        
        - stream=False: åª yield ä¸€æ¬¡å®Œæ•´å“åº”
        - stream=True: yield å¤šä¸ªå¢é‡ + æœ€åå®Œæ•´å“åº”
        """
        try:
            params = request.to_openai_format()
            params["model"] = self.get_model(request)
            params["stream"] = stream
            
            self._log_request(params)
            
            if stream:
                # æµå¼ç”Ÿæˆ
                stream_response = await asyncio.to_thread(
                    self.client.chat.completions.create,
                    **params
                )
                for response in self._process_stream(stream_response):
                    yield response
                    await asyncio.sleep(0)  # è®©å‡ºæ§åˆ¶æƒ
            else:
                # éæµå¼ç”Ÿæˆ
                response = await asyncio.to_thread(
                    self.client.chat.completions.create,
                    **params
                )
                result = self._parse_response(response)
                self._log_response(result)
                yield result
                
        except Exception as e:
            yield LlmResponse.from_error(str(e))
    
    # ==================== å“åº”è§£æ ====================
    
    def _parse_response(self, response: Any) -> LlmResponse:
        """è§£æ OpenAI éæµå¼å“åº”"""
        choice = response.choices[0]
        message = choice.message
        
        raw_content = message.content or ""
        clean_content, thinking = self._extract_thinking(raw_content)
        
        function_calls = []
        
        # 1. æ ‡å‡† OpenAI æ ¼å¼çš„ tool_calls
        if message.tool_calls:
            for tc in message.tool_calls:
                try:
                    args = json.loads(tc.function.arguments)
                except json.JSONDecodeError:
                    args = {}
                function_calls.append(FunctionCall(
                    id=tc.id,
                    name=tc.function.name,
                    args=args,
                ))
        
        # 2. MiniMax XML æ ¼å¼çš„å·¥å…·è°ƒç”¨ï¼ˆåœ¨ content ä¸­ï¼‰
        if not function_calls and self._has_xml_tool_calls(raw_content):
            xml_calls = self._parse_minimax_tool_calls(raw_content)
            function_calls.extend(xml_calls)
            clean_content = self._remove_minimax_tool_calls(clean_content)
        
        return LlmResponse(
            content=clean_content,
            function_calls=function_calls,
            thinking=thinking,
            raw_content=raw_content,
            finish_reason=choice.finish_reason,
            model=response.model,
            partial=False,
            usage={
                "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
                "completion_tokens": response.usage.completion_tokens if response.usage else 0,
                "total_tokens": response.usage.total_tokens if response.usage else 0,
            } if response.usage else {},
        )
    
    def _process_stream(self, stream: Any) -> Iterator[LlmResponse]:
        """å¤„ç†æµå¼å“åº”"""
        full_content = ""
        tool_calls_data: list[dict[str, Any]] = []
        finish_reason = None
        model_name = None
        chunk_index = 0
        
        thinking_filter = ThinkingFilter()
        
        for chunk in stream:
            if not chunk.choices:
                continue
            
            choice = chunk.choices[0]
            delta = choice.delta
            
            if chunk.model:
                model_name = chunk.model
            
            # å¤„ç†å†…å®¹
            if delta.content:
                full_content += delta.content
                filtered_delta = thinking_filter.process_delta(delta.content)
                
                if self.show_thinking:
                    yield LlmResponse.create_delta(delta.content, chunk_index)
                elif filtered_delta:
                    yield LlmResponse.create_delta(filtered_delta, chunk_index)
                chunk_index += 1
            
            # å¤„ç†å·¥å…·è°ƒç”¨
            if delta.tool_calls:
                for tc in delta.tool_calls:
                    tc_index = tc.index
                    tc_id = tc.id
                    tc_name = tc.function.name if tc.function else None
                    tc_args = tc.function.arguments if tc.function else ""
                    
                    if tc_index < len(tool_calls_data):
                        existing = tool_calls_data[tc_index]
                        if tc_args:
                            existing["arguments"] = existing.get("arguments", "") + tc_args
                        if tc_id:
                            existing["id"] = tc_id
                        if tc_name:
                            existing["name"] = tc_name
                    else:
                        tool_calls_data.append({
                            "id": tc_id,
                            "name": tc_name,
                            "arguments": tc_args or "",
                        })
            
            if choice.finish_reason:
                finish_reason = choice.finish_reason
        
        # å®Œæˆè¿‡æ»¤
        clean_content, thinking, remaining = thinking_filter.finalize()
        
        if not self.show_thinking and remaining:
            yield LlmResponse.create_delta(remaining, chunk_index)
        
        # è§£æå·¥å…·è°ƒç”¨
        function_calls = []
        for tc in tool_calls_data:
            if tc.get("name"):
                try:
                    args = json.loads(tc.get("arguments") or "{}")
                except json.JSONDecodeError:
                    args = {}
                function_calls.append(FunctionCall(
                    id=tc.get("id", "call_unknown"),
                    name=tc["name"],
                    args=args,
                ))
        
        # å¦‚æœæ²¡æœ‰æ ‡å‡†å·¥å…·è°ƒç”¨ï¼Œæ£€æŸ¥ MiniMax XML æ ¼å¼
        if not function_calls and self._has_xml_tool_calls(full_content):
            function_calls = self._parse_minimax_tool_calls(full_content)
            clean_content = self._remove_minimax_tool_calls(clean_content)
        
        # è¿”å›æœ€ç»ˆå®Œæ•´å“åº”
        final_response = LlmResponse(
            content=clean_content,
            function_calls=function_calls,
            thinking=thinking,
            raw_content=full_content,
            finish_reason=finish_reason,
            model=model_name or self.model,
            partial=False,
        )
        self._log_response(final_response)
        yield final_response
    
    # ==================== è¾…åŠ©æ–¹æ³• ====================
    
    def _extract_thinking(self, raw_content: str) -> tuple[str, str]:
        """æå–å¹¶åˆ†ç¦»æ€è€ƒå†…å®¹"""
        if not raw_content:
            return "", ""
        
        think_pattern = r'<think>(.*?)</think>'
        thinking_parts = re.findall(think_pattern, raw_content, re.DOTALL)
        clean_content = re.sub(think_pattern, '', raw_content, flags=re.DOTALL).strip()
        thinking_content = '\n'.join(part.strip() for part in thinking_parts) if thinking_parts else ''
        
        return clean_content, thinking_content
    
    def _parse_minimax_tool_calls(self, content: str) -> list[FunctionCall]:
        """è§£æ MiniMax æ¨¡å‹çš„ XML æ ¼å¼å·¥å…·è°ƒç”¨"""
        function_calls = []
        call_index = 0
        
        # æ ¼å¼ 1: <minimax:tool_call>...</minimax:tool_call>
        tool_call_pattern = r'<minimax:tool_call>(.*?)</minimax:tool_call>'
        for block in re.findall(tool_call_pattern, content, re.DOTALL):
            fc = self._parse_invoke_block(block, call_index)
            if fc:
                function_calls.append(fc)
                call_index += 1
        
        # æ ¼å¼ 2 & 3: ç‹¬ç«‹çš„ <invoke>...</invoke>
        remaining = re.sub(tool_call_pattern, '', content, flags=re.DOTALL)
        invoke_pattern = r'<invoke[^>]*>(.*?)</invoke>'
        for block in re.findall(invoke_pattern, remaining, re.DOTALL):
            fc = self._parse_invoke_block(block, call_index)
            if fc:
                function_calls.append(fc)
                call_index += 1
        
        return function_calls
    
    def _parse_invoke_block(self, block: str, index: int) -> FunctionCall | None:
        """è§£æå•ä¸ª invoke å—"""
        # æ ¼å¼ A: <invoke name="tool_name"><parameter name="...">...</parameter></invoke>
        invoke_name_match = re.search(r'<invoke\s+name="([^"]+)"', block)
        if invoke_name_match:
            func_name = invoke_name_match.group(1)
            param_pattern = r'<parameter\s+name="([^"]+)"[^>]*>(.*?)</parameter>'
            params = re.findall(param_pattern, block, re.DOTALL)
            args = {name: value.strip() for name, value in params}
            return FunctionCall(id=f"call_minimax_{index}", name=func_name, args=args)
        
        # æ ¼å¼ B: <invoke><tool_name><param1>value1</param1></tool_name></invoke>
        tool_match = re.search(r'<(\w+)>(.*?)</\1>', block, re.DOTALL)
        if tool_match:
            func_name = tool_match.group(1)
            inner_content = tool_match.group(2)
            
            args = {}
            param_matches = re.findall(r'<(\w+)>(.*?)</\1>', inner_content, re.DOTALL)
            for param_name, param_value in param_matches:
                args[param_name] = param_value.strip()
            
            # ç‰¹æ®Šå¤„ç†: transfer_to_agent çš„å‚æ•°æ˜ å°„
            if func_name == 'transfer_to_agent':
                if 'agent' in args:
                    args['agent_name'] = args.pop('agent')
                if 'args' in args:
                    args_content = args.pop('args')
                    nested = re.findall(r'<(\w+)>(.*?)</\1>', args_content, re.DOTALL)
                    if nested:
                        for param_name, param_value in nested:
                            args[param_name] = param_value.strip()
                    else:
                        args['reason'] = args_content.strip()
            
            return FunctionCall(id=f"call_minimax_{index}", name=func_name, args=args)
        
        return None
    
    def _has_xml_tool_calls(self, content: str) -> bool:
        """æ£€æŸ¥å†…å®¹æ˜¯å¦åŒ…å« XML æ ¼å¼çš„å·¥å…·è°ƒç”¨"""
        return '<minimax:tool_call>' in content or '<invoke>' in content or '<invoke ' in content
    
    def _remove_minimax_tool_calls(self, content: str) -> str:
        """ä» content ä¸­ç§»é™¤ MiniMax XML æ ¼å¼çš„å·¥å…·è°ƒç”¨"""
        clean = re.sub(r'<minimax:tool_call>.*?</minimax:tool_call>', '', content, flags=re.DOTALL)
        clean = re.sub(r'<invoke[^>]*>.*?</invoke>', '', clean, flags=re.DOTALL)
        return clean.strip()
    
    # ==================== æ—¥å¿— ====================
    
    def _log_request(self, params: dict[str, Any]) -> None:
        """æ‰“å° API è¯·æ±‚è¯¦æƒ…"""
        if not self.show_request:
            return
        
        level = self.log_level
        
        if level == "minimal":
            tools = params.get("tools", [])
            tool_names = [t.get("function", {}).get("name", "?") for t in tools]
            msgs = params.get("messages", [])
            last_user = next((m["content"][:50] for m in reversed(msgs) if m.get("role") == "user"), "")
            print(f"ğŸ“¤ INPUT | model={params.get('model')} | stream={params.get('stream')} | tools={tool_names} | user=\"{last_user}...\"")
            return
        
        print("\n" + "=" * 60)
        print("ğŸ“¤ LLM INPUT")
        print("=" * 60)
        print(f"ğŸ¤– Model: {params.get('model', 'N/A')} | Stream: {params.get('stream', False)}")
        
        tools = params.get("tools", [])
        if tools:
            tool_names = [t.get("function", {}).get("name", "?") for t in tools]
            print(f"ğŸ”§ Tools: {tool_names}")
            
            if level == "verbose":
                for t in tools:
                    func = t.get("function", {})
                    func_params = func.get("parameters", {})
                    print(f"   ğŸ“Œ {func.get('name')}: {func.get('description', '')[:60]}...")
                    if func_params.get("properties"):
                        print(f"      å‚æ•°: {list(func_params['properties'].keys())}")
        
        messages = params.get("messages", [])
        print(f"\nğŸ“ Messages ({len(messages)}):")
        
        for i, msg in enumerate(messages):
            role = msg.get("role", "unknown")
            content = msg.get("content", "")
            tool_calls = msg.get("tool_calls", [])
            
            if level == "normal":
                if role == "system":
                    preview = str(content).replace('\n', ' ')[:80]
                    print(f"  [{i+1}] SYSTEM: {preview}...")
                elif role == "user":
                    print(f"  [{i+1}] USER: {content}")
                elif role == "assistant":
                    if tool_calls:
                        tc_names = [tc.get("function", {}).get("name", "?") for tc in tool_calls]
                        print(f"  [{i+1}] ASSISTANT: [è°ƒç”¨å·¥å…·: {tc_names}]")
                    else:
                        preview = str(content).replace('\n', ' ')[:60]
                        print(f"  [{i+1}] ASSISTANT: {preview}...")
                elif role == "tool":
                    result = str(content)[:40]
                    print(f"  [{i+1}] TOOL: {result}...")
            else:
                print(f"\n  [{i+1}] ã€{role.upper()}ã€‘")
                if role == "assistant" and tool_calls:
                    if content:
                        for line in str(content).split('\n'):
                            print(f"      {line}")
                    print("      ğŸ”§ å·¥å…·è°ƒç”¨:")
                    for tc in tool_calls:
                        func = tc.get("function", {})
                        print(f"         â†’ {func.get('name')}({func.get('arguments', '{}')})")
                elif role == "tool":
                    print(f"      (call_id: {msg.get('tool_call_id', '')})")
                    print(f"      {content}")
                else:
                    if content:
                        for line in str(content).split('\n'):
                            print(f"      {line}")
        
        print("=" * 60)
    
    def _log_response(self, response: 'LlmResponse') -> None:
        """æ‰“å° API å“åº”è¯¦æƒ…"""
        if not self.show_request:
            return
        
        level = self.log_level
        
        if level == "minimal":
            content_preview = str(response.content or "").replace('\n', ' ')[:50]
            tool_names = [fc.name for fc in response.function_calls] if response.function_calls else []
            if tool_names:
                print(f"ğŸ“¥ OUTPUT | tools={tool_names} | content=\"{content_preview}...\"")
            else:
                print(f"ğŸ“¥ OUTPUT | content=\"{content_preview}...\"")
            return
        
        print("\n" + "=" * 60)
        print("ğŸ“¥ LLM OUTPUT")
        print("=" * 60)
        
        if level == "verbose":
            print(f"ğŸ¤– Model: {response.model} | Finish: {response.finish_reason}")
        
        if response.thinking:
            if self.show_thinking:
                if level == "verbose":
                    print(f"\nğŸ’­ Thinking:")
                    for line in response.thinking.split('\n'):
                        print(f"    {line}")
                else:
                    thinking_preview = response.thinking.replace('\n', ' ')[:100]
                    print(f"ğŸ’­ Thinking: {thinking_preview}...")
            else:
                print(f"ğŸ’­ Thinking: (å·²éšè—)")
        
        if response.content:
            print(f"\nğŸ“ Content:")
            if level == "verbose":
                for line in response.content.split('\n'):
                    print(f"    {line}")
            else:
                content = response.content.strip()
                if len(content) > 200:
                    print(f"    {content[:200]}...")
                    print(f"    (å…± {len(content)} å­—ç¬¦)")
                else:
                    print(f"    {content}")
        
        if response.function_calls:
            print(f"\nğŸ”§ Tool Calls:")
            for fc in response.function_calls:
                if level == "verbose":
                    print(f"    ğŸ“Œ {fc.name}")
                    print(f"       ID: {fc.id}")
                    print(f"       Args: {json.dumps(fc.args, ensure_ascii=False)}")
                else:
                    args_str = json.dumps(fc.args, ensure_ascii=False)
                    if len(args_str) > 60:
                        args_str = args_str[:60] + "..."
                    print(f"    â†’ {fc.name}({args_str})")
        
        if level == "verbose" and response.usage:
            print(f"\nğŸ“Š Usage: prompt={response.usage.get('prompt_tokens', 'N/A')} | completion={response.usage.get('completion_tokens', 'N/A')} | total={response.usage.get('total_tokens', 'N/A')}")
        
        print("=" * 60 + "\n")
