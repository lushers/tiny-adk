"""OpenAI å…¼å®¹çš„ LLM å®ç°"""

from __future__ import annotations

import asyncio
import json
import re
from typing import Any, AsyncIterator, Iterator

from .base_llm import BaseLlm
from .llm_request import LlmRequest
from .llm_response import LlmResponse, ToolCall


class ThinkingFilter:
    """æµå¼æ€è€ƒå†…å®¹è¿‡æ»¤å™¨ - å®æ—¶è¿‡æ»¤ <think> æ ‡ç­¾"""
    
    def __init__(self):
        self.buffer = ''
        self.in_thinking = False
        self.thinking_content = ''
        self.clean_content = ''
    
    def process_delta(self, delta: str) -> str:
        """å¤„ç†æµå¼å†…å®¹ç‰‡æ®µï¼Œè¿‡æ»¤ thinking å†…å®¹"""
        self.buffer += delta
        output = ''
        
        while self.buffer:
            if not self.in_thinking:
                think_start_idx = self.buffer.find('<think>')
                
                if think_start_idx == -1:
                    if len(self.buffer) > 10:
                        output_part = self.buffer[:-7]
                        output += output_part
                        self.clean_content += output_part
                        self.buffer = self.buffer[-7:]
                    break
                else:
                    if think_start_idx > 0:
                        output_part = self.buffer[:think_start_idx]
                        output += output_part
                        self.clean_content += output_part
                    self.buffer = self.buffer[think_start_idx + 7:]
                    self.in_thinking = True
            else:
                think_end_idx = self.buffer.find('</think>')
                
                if think_end_idx == -1:
                    if len(self.buffer) > 10:
                        thinking_part = self.buffer[:-8]
                        self.thinking_content += thinking_part
                        self.buffer = self.buffer[-8:]
                    break
                else:
                    self.thinking_content += self.buffer[:think_end_idx]
                    self.buffer = self.buffer[think_end_idx + 8:]
                    self.in_thinking = False
        
        return output
    
    def finalize(self) -> tuple[str, str, str]:
        """è¿”å› (clean_content, thinking_content, remaining_buffer)"""
        remaining = ''
        
        if self.buffer:
            if self.in_thinking:
                self.thinking_content += self.buffer
            else:
                remaining = self.buffer
                self.clean_content += self.buffer
            self.buffer = ''
        
        return self.clean_content.strip(), self.thinking_content.strip(), remaining


class OpenAILlm(BaseLlm):
    """
    OpenAI å…¼å®¹çš„ LLM å®ç°
    
    æ”¯æŒ:
    - OpenAI API
    - vLLM (OpenAI å…¼å®¹æ¨¡å¼)
    - å…¶ä»– OpenAI å…¼å®¹çš„ API
    
    Attributes:
        api_base: API åœ°å€
        api_key: API å¯†é’¥
        model: é»˜è®¤æ¨¡å‹åç§°
        show_thinking: æ˜¯å¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
        show_request: æ˜¯å¦æ˜¾ç¤º API è¯·æ±‚è¯¦æƒ…
    """
    
    def __init__(
        self,
        api_base: str | None = None,
        api_key: str | None = None,
        model: str = "",
        show_thinking: bool = False,
        show_request: bool = False,
        client: Any = None,
    ):
        super().__init__(model)
        self.api_base = api_base
        self.api_key = api_key
        self.show_thinking = show_thinking
        self.show_request = show_request
        self._client = client
    
    @property
    def client(self) -> Any:
        """è·å– OpenAI å®¢æˆ·ç«¯ï¼ˆæ‡’åŠ è½½ï¼‰"""
        if self._client is None:
            from openai import OpenAI
            self._client = OpenAI(
                base_url=self.api_base,
                api_key=self.api_key,
            )
        return self._client
    
    @classmethod
    def supported_models(cls) -> list[str]:
        return [
            r"gpt-.*",
            r"o1-.*",
            r"chatgpt-.*",
        ]
    
    def generate(self, request: LlmRequest) -> LlmResponse:
        """åŒæ­¥éæµå¼ç”Ÿæˆ"""
        try:
            params = request.to_openai_format()
            params["model"] = self.get_model(request)
            params["stream"] = False
            
            self._log_request(params)
            response = self.client.chat.completions.create(**params)
            return self._parse_response(response)
        except Exception as e:
            return LlmResponse.from_error(str(e))
    
    def generate_stream(self, request: LlmRequest) -> Iterator[LlmResponse]:
        """åŒæ­¥æµå¼ç”Ÿæˆ"""
        try:
            params = request.to_openai_format()
            params["model"] = self.get_model(request)
            params["stream"] = True
            
            self._log_request(params)
            stream = self.client.chat.completions.create(**params)
            yield from self._process_stream(stream)
        except Exception as e:
            yield LlmResponse.from_error(str(e))
    
    async def generate_async(self, request: LlmRequest) -> LlmResponse:
        """å¼‚æ­¥éæµå¼ç”Ÿæˆ"""
        try:
            params = request.to_openai_format()
            params["model"] = self.get_model(request)
            params["stream"] = False
            
            self._log_request(params)
            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡ŒåŒæ­¥è°ƒç”¨
            response = await asyncio.to_thread(
                self.client.chat.completions.create,
                **params
            )
            return self._parse_response(response)
        except Exception as e:
            return LlmResponse.from_error(str(e))
    
    async def generate_stream_async(
        self, request: LlmRequest
    ) -> AsyncIterator[LlmResponse]:
        """å¼‚æ­¥æµå¼ç”Ÿæˆ"""
        try:
            params = request.to_openai_format()
            params["model"] = self.get_model(request)
            params["stream"] = True
            
            self._log_request(params)
            stream = await asyncio.to_thread(
                self.client.chat.completions.create,
                **params
            )
            
            for response in self._process_stream(stream):
                yield response
                await asyncio.sleep(0)  # è®©å‡ºæ§åˆ¶æƒ
        except Exception as e:
            yield LlmResponse.from_error(str(e))
    
    def _parse_response(self, response: Any) -> LlmResponse:
        """è§£æ OpenAI å“åº”"""
        choice = response.choices[0]
        message = choice.message
        
        raw_content = message.content or ""
        clean_content, thinking = self._extract_thinking(raw_content)
        
        tool_calls = []
        if message.tool_calls:
            for tc in message.tool_calls:
                try:
                    args = json.loads(tc.function.arguments)
                except json.JSONDecodeError:
                    args = {}
                tool_calls.append(ToolCall(
                    id=tc.id,
                    name=tc.function.name,
                    arguments=args,
                ))
        
        return LlmResponse(
            content=clean_content,
            tool_calls=tool_calls,
            thinking=thinking,
            raw_content=raw_content,
            finish_reason=choice.finish_reason,
            model=response.model,
            usage={
                "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
                "completion_tokens": response.usage.completion_tokens if response.usage else 0,
                "total_tokens": response.usage.total_tokens if response.usage else 0,
            } if response.usage else {},
        )
    
    def _process_stream(self, stream: Any) -> Iterator[LlmResponse]:
        """å¤„ç†æµå¼å“åº”"""
        full_content = ""
        tool_calls_data: list[dict[str, Any]] = []
        finish_reason = None
        model_name = None
        chunk_index = 0
        
        thinking_filter = ThinkingFilter()
        
        for chunk in stream:
            if not chunk.choices:
                continue
            
            choice = chunk.choices[0]
            delta = choice.delta
            
            if chunk.model:
                model_name = chunk.model
            
            # å¤„ç†å†…å®¹
            if delta.content:
                full_content += delta.content
                filtered_delta = thinking_filter.process_delta(delta.content)
                
                if self.show_thinking:
                    yield LlmResponse.create_delta(delta.content, chunk_index)
                elif filtered_delta:
                    yield LlmResponse.create_delta(filtered_delta, chunk_index)
                chunk_index += 1
            
            # å¤„ç†å·¥å…·è°ƒç”¨
            if delta.tool_calls:
                for tc in delta.tool_calls:
                    tc_index = tc.index
                    tc_id = tc.id
                    tc_name = tc.function.name if tc.function else None
                    tc_args = tc.function.arguments if tc.function else ""
                    
                    if tc_index < len(tool_calls_data):
                        existing = tool_calls_data[tc_index]
                        if tc_args:
                            existing["arguments"] = existing.get("arguments", "") + tc_args
                        if tc_id:
                            existing["id"] = tc_id
                        if tc_name:
                            existing["name"] = tc_name
                    else:
                        tool_calls_data.append({
                            "id": tc_id,
                            "name": tc_name,
                            "arguments": tc_args or "",
                        })
            
            if choice.finish_reason:
                finish_reason = choice.finish_reason
        
        # å®Œæˆè¿‡æ»¤
        clean_content, thinking, remaining = thinking_filter.finalize()
        
        if not self.show_thinking and remaining:
            yield LlmResponse.create_delta(remaining, chunk_index)
        
        # è§£æå·¥å…·è°ƒç”¨
        tool_calls = []
        for tc in tool_calls_data:
            if tc.get("name"):
                try:
                    args = json.loads(tc.get("arguments") or "{}")
                except json.JSONDecodeError:
                    args = {}
                tool_calls.append(ToolCall(
                    id=tc.get("id", "call_unknown"),
                    name=tc["name"],
                    arguments=args,
                ))
        
        # è¿”å›æœ€ç»ˆå®Œæ•´å“åº”
        yield LlmResponse(
            content=clean_content,
            tool_calls=tool_calls,
            thinking=thinking,
            raw_content=full_content,
            finish_reason=finish_reason,
            model=model_name or self.model,
            partial=False,
        )
    
    def _extract_thinking(self, raw_content: str) -> tuple[str, str]:
        """æå–å¹¶åˆ†ç¦»æ€è€ƒå†…å®¹"""
        if not raw_content:
            return "", ""
        
        think_pattern = r'<think>(.*?)</think>'
        thinking_parts = re.findall(think_pattern, raw_content, re.DOTALL)
        clean_content = re.sub(think_pattern, '', raw_content, flags=re.DOTALL).strip()
        thinking_content = '\n'.join(part.strip() for part in thinking_parts) if thinking_parts else ''
        
        return clean_content, thinking_content
    
    def _log_request(self, params: dict[str, Any]) -> None:
        """æ‰“å° API è¯·æ±‚è¯¦æƒ…ï¼ˆè°ƒè¯•ç”¨ï¼‰"""
        if not self.show_request:
            return
        
        print("\n" + "=" * 60)
        print("ğŸ“¤ LLM API Request")
        print("=" * 60)
        print(f"ğŸ”— API Base: {self.api_base}")
        print(f"ğŸ¤– Model: {params.get('model', 'N/A')}")
        print(f"ğŸŒŠ Stream: {params.get('stream', False)}")
        
        # å…ˆæ‰“å°å·¥å…·å®šä¹‰
        tools = params.get("tools", [])
        if tools:
            print(f"\nğŸ”§ Tools ({len(tools)}):")
            for t in tools:
                func = t.get("function", {})
                print(f"  - {func.get('name', 'unknown')}: {func.get('description', 'N/A')[:50]}...")
        
        # å†æ‰“å°æ¶ˆæ¯
        messages = params.get("messages", [])
        print(f"\nğŸ“ Messages ({len(messages)}):")
        for i, msg in enumerate(messages):
            role = msg.get("role", "unknown")
            content = msg.get("content", "")
            tool_calls = msg.get("tool_calls", [])
            
            # å¤„ç† assistant è°ƒç”¨å·¥å…·çš„æƒ…å†µ
            if role == "assistant" and not content and tool_calls:
                print(f"  [{i+1}] {role}: [è°ƒç”¨å·¥å…·]")
                for tc in tool_calls:
                    func = tc.get("function", {})
                    tc_name = func.get("name", "?")
                    tc_args = func.get("arguments", "{}")
                    # å¦‚æœ arguments æ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºæ›´å‹å¥½çš„æ ¼å¼
                    if isinstance(tc_args, str) and len(tc_args) > 100:
                        tc_args = tc_args[:100] + "..."
                    print(f"        â†’ {tc_name}({tc_args})")
            # å¤„ç† tool å“åº”æ¶ˆæ¯
            elif role == "tool":
                tool_call_id = msg.get("tool_call_id", "")
                if isinstance(content, str) and len(content) > 100:
                    content = content[:100] + "..."
                print(f"  [{i+1}] {role} ({tool_call_id[:12]}...): {content}")
            else:
                # æˆªæ–­è¿‡é•¿çš„å†…å®¹
                if isinstance(content, str) and len(content) > 200:
                    content = content[:200] + "..."
                print(f"  [{i+1}] {role}: {content}")
        
        print("=" * 60 + "\n")

